---
title: "Classification"
format: html
---

Classification models are used when you know the categories, or labels, that you wish to use on your data. Both classification and regression are forms of **supervised** learning.

Supervised learning is a type of machine learning where the model learns from a labeled data set‚Äîthat is, data that has both **input features (X)** and known **target outputs (Y)**.

The goal is to learn a function that maps inputs to outputs so it can make predictions on new, unseen data.

A practical example of supervised learning and classification is spam email detection. We want to automatically classify incoming emails as **"spam"** or **"not spam."** The **inputs (features) c**ould include:

-   Presence of certain keywords (like "free", "winner", etc.)

-   Sender's email address

-   Number of links or attachments

-   Email length

-   Frequency of punctuation like "!!!" or "\$\$\$"

These act like clues to let the computer know that a specific email could be spam. You can understand how this is modeled on human thinking. We also look for patterns that will lead us to a conclusion.

The **outputs (labels)** in this case would be `1` for spam, and `0` for not spam (aka ham). If you have email a classification model is likely at work on your account, separating the "spam" from the "ham."

Common supervised learning models used for classifying data are:

**Logistic Regression**

This is a statistical method used to model the relationship between a binary dependent variable and one or more independent variables. An answer is either "yes" or "no." It estimates the probability that the dependent variable equals a certain value (usually 1), using a logistic function (a sigmoid, or s-shaped, curve).

To run a logistic regression in R, use the `glm()` function with an argument of `family = binomial`. This fits a generalized linear model where the dependent variable is binary (0/1). You should check assumptions, multicollinearity, and goodness-of-fit afterward.

**Naive Bayes (N-B)**

N-B is a probabilistic machine learning algorithm based on Bayes‚Äô Theorem. It's particularly used for classification tasks, and it's called ‚Äúnaive‚Äù because it assumes independence between features ‚Äî which is rarely true in practice, but surprisingly effective in many cases.

For example, in spam detection, N-B would assume that the presence of the word ‚Äúfree‚Äù is independent of ‚Äúmoney‚Äù ‚Äî given the email is spam. Not usually true, but this simplifies the math **a lot**.

Sure! Here's a simple example of implementing **Naive Bayes** in **R** using the built-in `iris` dataset and the `e1071` package.

------------------------------------------------------------------------

### üì¶ Step 1: Install and load the required package

``` r
install.packages("e1071")  # Only run this once
library(e1071)
```

------------------------------------------------------------------------

### üå∏ Step 2: Prepare the data

We‚Äôll use the famous `iris` dataset, which classifies flowers into three species.

``` r
data(iris)

# Let's split the data into training and test sets
set.seed(123)  # For reproducibility
indexes <- sample(1:nrow(iris), 0.7 * nrow(iris))
train_data <- iris[indexes, ]
test_data <- iris[-indexes, ]
```

------------------------------------------------------------------------

### üß† Step 3: Train the Naive Bayes model

``` r
model <- naiveBayes(Species ~ ., data = train_data)
print(model)
```

------------------------------------------------------------------------

### üß™ Step 4: Make predictions

``` r
predictions <- predict(model, test_data)
```

------------------------------------------------------------------------

### üìä Step 5: Evaluate the model

``` r
confusion_matrix <- table(Predicted = predictions, Actual = test_data$Species)
print(confusion_matrix)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
```

------------------------------------------------------------------------

This is a basic classification task using Naive Bayes. You can explore further with different datasets, feature selection, and using `caret` or `mlr3` for more structured workflows.

Would you like an example using text data (like spam detection), or is this enough for now?

Get \$60 free credit to access GPT-4, Claude 3, Gemini, Grok & more ‚Äî just \$5.99/month (billed annually) or \$6.99 billed monthly: <https://responxe.com/landing/ref/REF_M9C0953>

**Support Vector Machines**

SVM is a supervised machine learning algorithm primarily used for classification, but it can also handle regression tasks. This means that

At its core, an SVM tries to find the best boundary (hyperplane) that separates data points of different classes in the feature space. SVM tries to **maximize the margin** between data points of different classes. The margin is the distance between the hyperplane and the **nearest points from each class**, which are called **support vectors**.

**Random Forests**

These are essentially a collection of decision trees‚Äîhence the name "forest." Each tree is trained on a slightly different subset of the data, and their outputs are aggregated to make the final prediction.

For classification, the forest outputs the class that has the majority vote among the trees.

For regression, it averages the predictions of all the trees.

But what about it is "random?"

1.  **Bootstrap sampling (Bagging)**: Each tree is trained on a **random sample** (with replacement) of the training data.

2.  **Random feature selection**: When splitting nodes, each tree considers only a **random subset of features** instead of all features. This adds diversity and reduces correlation among trees.

**Neural Networks**

A Neural Networks (NN) machine learning model inspired by how the human brain works. It is composed of layers of interconnected nodes (called **neurons**) that can learn to recognize patterns in data.

In essence, a neural network is a **function approximator** that maps input data to an output (like predicting if a person has diabetes or classifying an image).
