---
title: "Clustering"
format: html
---

Clustering is used when you want to classify your data but you are not sure how. Clustering is a form of **unsupervised** learning.

### **K-Means**

One of the most popular types of clustering models is **K-means** clustering.

K-Means groups data into clusters where each cluster is roughly circular (in 2D) or spherical (in higher dimensions) around its "centroid." A centroid is the geometric center or the "average" point of a set of points in a multidimensional space.

Thus, a key assumption is that these spherical clusters are of equal variance. K-Means uses Euclidean distance to assign points to the nearest cluster center â€” and all points that are equidistant to a center form a hypersphere. A hypersphere is the generalization of a circle (2D) and sphere (3D) to n-dimensional space.

ðŸ“Œ Key point- this makes clusters are shaped like blobs around the center â€” not elongated or skewed.

**Strengths**: Fast, simple, works well with large datasets.

**Limitations**: Requires the number of clusters (`k`) to be specified; not great with non-spherical or varying-density clusters.

R package: \[`stats::kmeans()`\], also `cluster` and `factoextra` for visualization.

### **Hierarchical**

Another popular type of clustering model is **Hierarchical** clustering.

**Types**: Agglomerative (bottom-up) and divisive (top-down).

Here is an easy way to understand a **bottom-up clustering method**.

**Start with every data point as its own cluster**.

**Find the two closest clusters** (based on a distance metric like Euclidean distance).

**Merge them into one cluster**.

**Repeat steps 2 and 3** until all data points are merged into a single cluster (or until a stopping condition is met). You can **cut the dendrogram** at any level to choose the number of clusters you want.

Itâ€™s a good choice when you want to **understand the hierarchy or nested structure** of your data.

**Output**: Dendrogram (tree of clusters).

**Strengths**: Does not require predefined `k`; good for nested structures.

**Limitations**: Computationally expensive for large datasets.

R package: `stats::hclust()`, `dendextend`, `cluster`.

### **DBSCAN**

Still another popular type of clustering mode is **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** clustering.

âœ… Why is it popular?

-   There is no need to specify the number of clusters in advance (unlike K-means).

-   Can find arbitrarily shaped clusters, not just spherical ones.

-   Robust to outliers â€” it can identify noise points as a separate class.

Core Concepts Îµ (epsilon): Radius around a point to look for neighboring points.

MinPts: Minimum number of points within Îµ radius to form a dense region.

Core Point: Has at least MinPts within Îµ.

Border Point: Within Îµ of a core point but has fewer than MinPts.

Noise Point: Not a core or border point.

Let's do an example of DBSCAN. The `iris` data set is often used, even though it is already labeled.

```{r}
#| message: false
#| warning: false
#| output: true
# Load necessary packages
install.packages("dbscan")
install.packages("ggplot2")
library(dbscan)
library(ggplot2)

# Load the iris dataset and check out the first six observations
data(iris)
head(iris)

# Use only the numeric features (remove the species label)
iris_data <- iris[, 1:4]

# Scale the data (DBSCAN is sensitive to scale)
iris_scaled <- scale(iris_data)

# Plot kNN distances to estimate a good value for eps
kNNdistplot(iris_scaled, k = 4)
abline(h = 0.6, lty = 2)  # use your visual judgment here for eps

```

This plot helps you choose a good `eps` value. You're looking for the "elbow" of the curve.

```{r}
#| message: false
#| warning: false
#| output: true
# Run DBSCAN with eps and minPts
db <- dbscan(iris_scaled, eps = 0.6, minPts = 4)

# Check clustering results
db$cluster  # Cluster labels (0 means noise)

# Add cluster labels to the original data
iris$Cluster <- as.factor(db$cluster)

# Visualize the clusters (using first two PCA components for clarity)
pca <- prcomp(iris_scaled)
iris_pca <- data.frame(pca$x[,1:2], Cluster = iris$Cluster)

ggplot(iris_pca, aes(PC1, PC2, color = Cluster)) +
  geom_point(size = 2) +
  labs(title = "DBSCAN Clustering on Iris Dataset (PCA Projection)")
```

### ðŸ“ Notes

**`eps`** (epsilon): max distance between two samples to be considered neighbors.

**`minPts`**: minimum number of points required to form a dense region.

The elbow/k-distance plot helps you pick a good `eps`.
