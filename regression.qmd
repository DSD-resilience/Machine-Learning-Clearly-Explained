---
title: "Linear and Logistic Regression"
---

**Regression** is a way to find a relationship between variables and use that relationship to make predictions.

For example:

-   If you know the **size of a house**, regression can help predict its **price**.

-   If you know a **studentâ€™s study hours**, regression can estimate their **exam score**.

Itâ€™s like drawing a **trend line** through data points to see how one thing changes with another. ðŸ“ˆðŸ˜Š

**Linear regression** is one of the most useful and popular forecasting tools available. Quite simply, it assumes that there is a linear relationship between one variable and a second variable. If you plot data for these two variables on a graph, you can draw a line through it and make a decent guess about what the second variable will be depending on the value of the first variable.

To learn more technical aspects of linear regression, visit [the Wikipedia entry](https://en.wikipedia.org/wiki/Linear_regression){.uri}.

An initial model for "simple linear regression" is easy to build in R. This may not seem that impressive at first, but we'll examine its technical output shortly.

Load the data set and build a linear model:

```{r}
#| message: false
#| warning: false
#| output: false
# Load dataset
data(iris)

# Iris is a common data solution for testing models because of its relative simplicity
# Fit linear regression model
model <- lm(Sepal.Length ~ Petal.Length, data = iris)
```

Load the necessary library for the chart:

```{r}
#| message: false
#| warning: false
#| output: false
# ggplot2 makes great charts!
install.packages('ggplot2')
library(ggplot2)
```

Now build the chart that displays the relationship between variables:

```{r}
#| message: false
#| warning: false
#| output: true
# Create scatter plot with regression line
ggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) +
  geom_point(color = "blue", alpha = 0.6) +  # Scatter points
  geom_smooth(method = "lm", color = "red", fill = "pink", se = TRUE) +  # Regression line
  labs(title = "Relationship Between Sepal Length and Petal Length",
       subtitle = "A simple linear regression model",
       x = "Petal Length",
       y = "Sepal Length") +
  theme_minimal()
```

The variable Petal Length is on the x-axis (horizontal) and is called the "independent" variable. The variable Sepal Length is on the y-axis (vertical) and is called the "dependent" variable. As you might guess, the objective of this "model" (a simulation of reality) is to see how much influence Petal Length may have on Sepal Length. We don't know if they are related at all at first, but by examining the data we can tease out that perhaps we can draw a straight line through the data that explains a possible relationship. In this case we see that, generally speaking, the longer the petal length is on an iris, the longer sepal length will be. Sometimes these relationships are intuitive, such as a taller person being heavier than a short person, but not always. Generally, we use data science to look for non-obvious relationships and performing a linear regression is just one tool.

Now that we have an intuitive grasp on the subject from the visualization, let's look at the statistical facts about this model.

```{r}
#| message: false
#| warning: false
#| output: false
# Display model summary
summary(model)
```

**Logistic regression** is also quite useful. It is a statistical method used for binary classification, where the outcome variable has two possible values (e.g., 0 or 1, yes or no, true or false). It models the probability of one class occurring using the logistic (sigmoid) function, which maps any real-valued input to a value between 0 and 1.

Logistic regression is commonly used in fields like medicine, finance, and social sciences for tasks such as disease prediction, credit scoring, and customer retention analysis.

Let's predict whether a car has **automatic (am = 0) or manual (am = 1) transmission** based on its **miles per gallon (mpg)**:

```{r}
# Load dataset
data(mtcars)

# Convert 'am' to a factor (0 = automatic, 1 = manual)
mtcars$am <- as.factor(mtcars$am)

# Fit a logistic regression model
model <- glm(am ~ mpg, data = mtcars, family = binomial)

# Summary of the model
summary(model)
```

-   The model predicts whether a car is **manual (1) or automatic (0)** based on `mpg`.

<!-- -->

-   The coefficient of `mpg` tells us whether increasing `mpg` makes a car more likely to have a manual transmission.

<!-- -->

-   The output is in **log-odds**, but we can convert it to probabilities.

In R 4.5.0, you can use the **`palmerpenguins`** dataset, which is a built-in dataset (after loading the appropriate package), to perform regression analysis. Here's a simple example using **linear regression** to predict penguin **body mass** from **flipper length**.

First, we'll make sure to have the `palmerpenguins` package installed and loaded, just in case someone has not updated to R4.5.0:

```{r}
# Install if you haven't already
install.packages("palmerpenguins")

# Load the package
library(palmerpenguins)
```

Then we would build the model:

```{r}

# Load the data
data("penguins")

# Remove rows with missing values
penguins_clean <- na.omit(penguins)

# Fit a linear regression model: body_mass_g ~ flipper_length_mm
model <- lm(body_mass_g ~ flipper_length_mm, data = penguins_clean)

# Show summary of the model
summary(model)
```

### Explanation:

-   `lm()` fits a linear regression model.

-   `body_mass_g` is the response variable.

-   `flipper_length_mm` is the predictor.

What the model demonstrates is that for every 1 mm increase in flipper length, the predicted body mass increases by approximately 58.4 grams.

Here is how you could visualize the model:

```{r}
# Plot
plot(penguins_clean$flipper_length_mm, penguins_clean$body_mass_g,
     main = "Body Mass vs Flipper Length",
     xlab = "Flipper Length (mm)",
     ylab = "Body Mass (g)",
     pch = 19, col = "steelblue")

# Add regression line
abline(model, col = "red", lwd = 2)
```
