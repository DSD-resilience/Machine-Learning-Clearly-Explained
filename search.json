[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification models are used when you know the categories, or labels, that you wish to use. Both classification and regression are forms of supervised learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear and Logistic Regression",
    "section": "",
    "text": "Regression is a way to find a relationship between variables and use that relationship to make predictions.\nFor example:\n\nIf you know the size of a house, regression can help predict its price.\nIf you know a student‚Äôs study hours, regression can estimate their exam score.\n\nIt‚Äôs like drawing a trend line through data points to see how one thing changes with another. üìàüòä\nLinear regression is one of the most useful and popular forecasting tools available. Quite simply, it assumes that there is a linear relationship between one variable and a second variable. If you plot data for these two variables on a graph, you can draw a line through it and make a decent guess about what the second variable will be depending on the value of the first variable.\nTo learn more technical aspects of linear regression, visit the Wikipedia entry.\nAn initial model for ‚Äúsimple linear regression‚Äù is easy to build in R. This may not seem that impressive on first blush, but we‚Äôll examine its technical output shortly.\n\n# Load dataset\ndata(iris)\n\n# Fit linear regression model\nmodel &lt;- lm(Sepal.Length ~ Petal.Length, data = iris)\n\n\n# Display a visualization that helps explain the model\n# Load necessary library\ninstall.packages('ggplot2')\n\n# Downloading packages -------------------------------------------------------\n- Downloading ggplot2 from CRAN ...             OK [4.7 Mb in 0.39s]\n- Downloading gtable from CRAN ...              OK [213 Kb in 0.47s]\n- Downloading isoband from CRAN ...             OK [1.6 Mb in 0.44s]\n- Downloading scales from CRAN ...              OK [682.7 Kb in 0.37s]\n- Downloading farver from CRAN ...              OK [1.4 Mb in 0.51s]\n- Downloading labeling from CRAN ...            OK [57.8 Kb in 0.4s]\n- Downloading munsell from CRAN ...             OK [235.8 Kb in 0.4s]\n- Downloading colorspace from CRAN ...          OK [2.5 Mb in 0.45s]\n- Downloading RColorBrewer from CRAN ...        OK [51.8 Kb in 0.33s]\n- Downloading viridisLite from CRAN ...         OK [1.2 Mb in 0.38s]\n- Downloading tibble from CRAN ...              OK [656 Kb in 0.34s]\n- Downloading fansi from CRAN ...               OK [299.4 Kb in 0.32s]\n- Downloading magrittr from CRAN ...            OK [215.3 Kb in 0.32s]\n- Downloading pillar from CRAN ...              OK [634.2 Kb in 0.34s]\n- Downloading utf8 from CRAN ...                OK [143.4 Kb in 0.35s]\n- Downloading vctrs from CRAN ...               OK [1.2 Mb in 0.34s]\n- Downloading pkgconfig from CRAN ...           OK [17.1 Kb in 0.33s]\n- Downloading withr from CRAN ...               OK [211.6 Kb in 0.35s]\nSuccessfully downloaded 18 packages in 10 seconds.\n\nThe following package(s) will be installed:\n- colorspace   [2.1-1]\n- fansi        [1.0.6]\n- farver       [2.1.2]\n- ggplot2      [3.5.1]\n- gtable       [0.3.6]\n- isoband      [0.2.7]\n- labeling     [0.4.3]\n- magrittr     [2.0.3]\n- munsell      [0.5.1]\n- pillar       [1.10.1]\n- pkgconfig    [2.0.3]\n- RColorBrewer [1.1-3]\n- scales       [1.3.0]\n- tibble       [3.2.1]\n- utf8         [1.2.4]\n- vctrs        [0.6.5]\n- viridisLite  [0.4.2]\n- withr        [3.0.2]\nThese packages will be installed into \"~/work/Machine-Learning-Clearly-Explained/Machine-Learning-Clearly-Explained/renv/library/R-4.2/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing gtable ...                         OK [installed binary and cached in 0.34s]\n- Installing isoband ...                        OK [installed binary and cached in 0.24s]\n- Installing farver ...                         OK [installed binary and cached in 0.23s]\n- Installing labeling ...                       OK [installed binary and cached in 0.2s]\n- Installing colorspace ...                     OK [installed binary and cached in 0.3s]\n- Installing munsell ...                        OK [installed binary and cached in 0.25s]\n- Installing RColorBrewer ...                   OK [installed binary and cached in 0.19s]\n- Installing viridisLite ...                    OK [installed binary and cached in 0.22s]\n- Installing scales ...                         OK [installed binary and cached in 0.37s]\n- Installing fansi ...                          OK [installed binary and cached in 0.2s]\n- Installing magrittr ...                       OK [installed binary and cached in 0.2s]\n- Installing utf8 ...                           OK [installed binary and cached in 0.2s]\n- Installing vctrs ...                          OK [installed binary and cached in 0.34s]\n- Installing pillar ...                         OK [installed binary and cached in 0.41s]\n- Installing pkgconfig ...                      OK [installed binary and cached in 0.2s]\n- Installing tibble ...                         OK [installed binary and cached in 0.46s]\n- Installing withr ...                          OK [installed binary and cached in 0.2s]\n- Installing ggplot2 ...                        OK [installed binary and cached in 0.64s]\nSuccessfully installed 18 packages in 5.7 seconds.\n\nlibrary(ggplot2)\n\n# Create scatter plot with regression line\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) +\n  geom_point(color = \"blue\", alpha = 0.6) +  # Scatter points\n  geom_smooth(method = \"lm\", color = \"red\", fill = \"pink\", se = TRUE) +  # Regression line\n  labs(title = \"Relationship Between Sepal Length and Petal Length\",\n       subtitle = \"A simple linear regression model\",\n       x = \"Petal Length\",\n       y = \"Sepal Length\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe variable Petal Length is on the x-axis (horizontal) and is called the ‚Äúindependent‚Äù variable. The variable Sepal Length is on the y-axis (vertical) and is called the ‚Äúdependent‚Äù variable. As you might guess, the objective of this ‚Äúmodel‚Äù (a simulation of reality) is to see how much influence Petal Length may have on Sepal Length. We don‚Äôt know if they are related at all at first, but by examining the data we can tease out that perhaps we can draw a straight line through the data that explains a possible relationship. In this case we see that, generally speaking, the longer the petal length is on an iris, the longer sepal length will be. Sometimes these relationships are intuitive, such as a taller person being heavier than a short person, but not always. Generally, we use data science to look for non-obvious relationships and performing a linear regression is just one tool.\nNow that we have an intuitive grasp on the subject from the visualization, let‚Äôs look at the statistical facts about this model.\n\n# Display model summary\nsummary(model)\n\nLogistic regression is also quite useful. It is a statistical method used for binary classification, where the outcome variable has two possible values (e.g., 0 or 1, yes or no, true or false). It models the probability of one class occurring using the logistic (sigmoid) function, which maps any real-valued input to a value between 0 and 1.\nLogistic regression is commonly used in fields like medicine, finance, and social sciences for tasks such as disease prediction, credit scoring, and customer retention analysis.\nLet‚Äôs predict whether a car has automatic (am = 0) or manual (am = 1) transmission based on its miles per gallon (mpg):\n\n# Load dataset\ndata(mtcars)\n\n# Convert 'am' to a factor (0 = automatic, 1 = manual)\nmtcars$am &lt;- as.factor(mtcars$am)\n\n# Fit a logistic regression model\nmodel &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nglm(formula = am ~ mpg, family = binomial, data = mtcars)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5701  -0.7531  -0.4245   0.5866   2.0617  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -6.6035     2.3514  -2.808  0.00498 **\nmpg           0.3070     0.1148   2.673  0.00751 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 29.675  on 30  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe model predicts whether a car is manual (1) or automatic (0) based on mpg.\n\n\n\nThe coefficient of mpg tells us whether increasing mpg makes a car more likely to have a manual transmission.\n\n\n\nThe output is in log-odds, but we can convert it to probabilities."
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is used when you want to classify your data but you are not sure how. Clustering is a form of unsupervised learning."
  }
]