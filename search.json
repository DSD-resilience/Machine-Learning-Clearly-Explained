[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Machine learning is one of the most important developments in computer science. It has driven modern progress in many ways, through improving medical research, construction methods, urban planning, and most any field that you can name that generates and consumes data.\nFundamentally, it is a way for computers to learn from data and improve at tasks without being explicitly told what to do step by step.\nInstead of writing rules by hand, we give the computer a lot of examples, and it figures out patterns or rules on its own.\nFor example:\nIf you want a computer to recognize pictures of cats, instead of coding all the features of a cat (like fur, ears, eyes, tail‚Ä¶), you show it many labeled pictures of cats and non-cats.\nOver time, it ‚Äúlearns‚Äù what features make a cat picture likely, based on those examples.\nBut that is not all. It depends on what models you utilize, but a computer doesn‚Äôt even need labels to recognize patterns in your data.\nIf you want a computer to tell you what marketing might work best, then you can give it the data from several customers. This is very common in today‚Äôs society and used in almost every industry.\nOver time, the computer ‚Äúlearns‚Äù what features are distinct from customer to customer and labels the customer by what marketing could work best and result in higher sales.\nBoth of these instances are kind of like teaching by example rather than instruction.\nMachine learning works because data contains patterns, and if you can model those patterns mathematically, you can generalize to new, unseen data.\nIt works well when:\nIn math terms, it‚Äôs about finding a function f(x) approximates y, where:\nTo apply machine learning, you usually need three things:"
  },
  {
    "objectID": "index.html#where-is-ml-useful",
    "href": "index.html#where-is-ml-useful",
    "title": "Introduction",
    "section": "üí° Where Is ML Useful?",
    "text": "üí° Where Is ML Useful?\nMachine learning is used almost everywhere you can collect data and automatize processes. Even something as human as farming is going to rely on better weather forecasting and production reports. Here are some broad categories:\n\nRegression ‚Äì predicting prices, forecasting demand\nClassification ‚Äì spam detection, disease diagnosis, fraud detection\nClustering ‚Äì segmenting customers, grouping similar documents\nRecommendation ‚Äì Netflix, YouTube, Amazon suggestions\nComputer Vision ‚Äì self-driving cars, facial recognition\nNatural Language Processing ‚Äì chatbots, translation, sentiment analysis"
  },
  {
    "objectID": "index.html#key-takeaway",
    "href": "index.html#key-takeaway",
    "title": "Introduction",
    "section": "üéØ Key Takeaway",
    "text": "üéØ Key Takeaway\n\nMachine learning is about using data to make smarter decisions!\nWith the right tools and data, we can teach computers to help us in ways that were once impossible ‚Äî from personalized healthcare to AI-powered assistants."
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is used when you want to classify your data but you are not sure how. Clustering is a form of unsupervised learning.\n\nK-Means\nOne of the most popular types of clustering models is K-means clustering.\nK-Means groups data into clusters where each cluster is roughly circular (in 2D) or spherical (in higher dimensions) around its ‚Äúcentroid.‚Äù A centroid is the geometric center or the ‚Äúaverage‚Äù point of a set of points in a multidimensional space.\nThus, a key assumption is that these spherical clusters are of equal variance. K-Means uses Euclidean distance to assign points to the nearest cluster center ‚Äî and all points that are equidistant to a center form a hypersphere. A hypersphere is the generalization of a circle (2D) and sphere (3D) to n-dimensional space.\nüìå Key point- this makes clusters are shaped like blobs around the center ‚Äî not elongated or skewed.\nStrengths: Fast, simple, works well with large datasets.\nLimitations: Requires the number of clusters (k) to be specified; not great with non-spherical or varying-density clusters.\nR package: [stats::kmeans()], also cluster and factoextra for visualization.\n\n\nHierarchical\nAnother popular type of clustering model is Hierarchical clustering. There are two methods: Agglomerative (bottom-up) or divisive (top-down).\nHere is an easy way to understand a bottom-up clustering method. Start with every data point as its own cluster. Find the two closest clusters (based on a distance metric like Euclidean distance), merge them into one cluster. Repeat this until all data points are merged into a single cluster (or until a stopping condition is met).\nIt‚Äôs a good choice when you want to understand the hierarchy or nested structure of your data.\nOutput: Dendrogram (tree of clusters).\nStrengths: Does not require predefined k; good for nested structures.\nLimitations: Computationally expensive for large datasets.\nR package: stats::hclust(), dendextend, cluster.\n\n\nDBSCAN\nStill another popular type of clustering mode is DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering.\n‚úÖ Why is it popular?\n\nThere is no need to specify the number of clusters in advance (unlike K-means).\nCan find arbitrarily shaped clusters, not just spherical ones.\nRobust to outliers ‚Äî it can identify noise points as a separate class.\n\nCore Concepts Œµ (epsilon): Radius around a point to look for neighboring points.\nMinPts: Minimum number of points within Œµ radius to form a dense region.\nCore Point: Has at least MinPts within Œµ.\nBorder Point: Within Œµ of a core point but has fewer than MinPts.\nNoise Point: Not a core or border point.\nLet‚Äôs do an example of DBSCAN. The iris data set is often used, even though it is already labeled.\n\n# Load necessary packages\ninstall.packages(\"dbscan\")\n\n# Downloading packages -------------------------------------------------------\n- Downloading dbscan from CRAN ...              OK [2.8 Mb in 0.47s]\n- Downloading generics from CRAN ...            OK [74.3 Kb in 0.15s]\n- Downloading Rcpp from CRAN ...                OK [2 Mb in 0.34s]\nSuccessfully downloaded 3 packages in 1.5 seconds.\n\nThe following package(s) will be installed:\n- dbscan   [1.2.2]\n- generics [0.1.4]\n- Rcpp     [1.0.14]\nThese packages will be installed into \"~/work/Machine-Learning-Clearly-Explained/Machine-Learning-Clearly-Explained/renv/library/R-4.2/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing generics ...                       OK [installed binary and cached in 0.2s]\n- Installing Rcpp ...                           OK [installed binary and cached in 0.41s]\n- Installing dbscan ...                         OK [installed binary and cached in 0.34s]\nSuccessfully installed 3 packages in 1.1 seconds.\n\ninstall.packages(\"ggplot2\")\n\nThe following package(s) will be installed:\n- ggplot2 [3.5.2]\nThese packages will be installed into \"~/work/Machine-Learning-Clearly-Explained/Machine-Learning-Clearly-Explained/renv/library/R-4.2/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggplot2 ...                        OK [linked from cache]\nSuccessfully installed 1 package in 4.6 milliseconds.\n\nlibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nlibrary(ggplot2)\n\n# Load the iris dataset and check out the first six observations\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# Use only the numeric features (remove the species label)\niris_data &lt;- iris[, 1:4]\n\n# Scale the data (DBSCAN is sensitive to scale)\niris_scaled &lt;- scale(iris_data)\n\n# Plot kNN distances to estimate a good value for eps\nkNNdistplot(iris_scaled, k = 4)\nabline(h = 0.6, lty = 2)  # use your visual judgment here for eps\n\n\n\n\n\n\n\n\nThis plot helps you choose a good eps value. You‚Äôre looking for the ‚Äúelbow‚Äù of the curve.\n\n# Run DBSCAN with eps and minPts\ndb &lt;- dbscan(iris_scaled, eps = 0.6, minPts = 4)\n\n# Check clustering results\ndb$cluster  # Cluster labels (0 means noise)\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 0 2 2 0 2 0 2 2 2 2 2 0 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 0 0 2 0 0 2\n[112] 2 2 2 2 2 2 0 0 0 2 2 0 2 2 0 2 2 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\n# Add cluster labels to the original data\niris$Cluster &lt;- as.factor(db$cluster)\n\n# Visualize the clusters (using first two PCA components for clarity)\npca &lt;- prcomp(iris_scaled)\niris_pca &lt;- data.frame(pca$x[,1:2], Cluster = iris$Cluster)\n\nggplot(iris_pca, aes(PC1, PC2, color = Cluster)) +\n  geom_point(size = 2) +\n  labs(title = \"DBSCAN Clustering on Iris Dataset (PCA Projection)\")\n\n\n\n\n\n\n\n\n\n\nüìù Notes\neps (epsilon): max distance between two samples to be considered neighbors.\nminPts: minimum number of points required to form a dense region.\nThe elbow/k-distance plot helps you pick a good eps."
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification models are used when you know the categories, or labels, that you wish to use on your data. Both classification and regression are forms of supervised learning.\nSupervised learning is a type of machine learning where the model learns from a labeled data set‚Äîthat is, data that has both input features (X) and known target outputs (Y).\nThe goal is to learn a function that maps inputs to outputs so it can make predictions on new, unseen data.\nA practical example of supervised learning and classification is spam email detection. We want to automatically classify incoming emails as ‚Äúspam‚Äù or ‚Äúnot spam.‚Äù The inputs (features) could include:\n\nPresence of certain keywords (like ‚Äúfree‚Äù, ‚Äúwinner‚Äù, etc.)\nSender‚Äôs email address\nNumber of links or attachments\nEmail length\nFrequency of punctuation like ‚Äú!!!‚Äù or ‚Äú$$$‚Äù\n\nThese act like clues to let the computer know that a specific email could be spam. You can understand how this is modeled on human thinking. We also look for patterns that will lead us to a conclusion.\nThe outputs (labels) in this case would be 1 for spam, and 0 for not spam (aka ham). If you have email a classification model is likely at work on your account, separating the ‚Äúspam‚Äù from the ‚Äúham.‚Äù\nCommon supervised learning models used for classifying data are:\nLogistic Regression\nThis is a statistical method used to model the relationship between a binary dependent variable and one or more independent variables. An answer is either ‚Äúyes‚Äù or ‚Äúno.‚Äù It estimates the probability that the dependent variable equals a certain value (usually 1), using a logistic function (a sigmoid, or s-shaped, curve).\nTo run a logistic regression in R, use the glm() function with an argument of family = binomial. This fits a generalized linear model where the dependent variable is binary (0/1). You should check assumptions, multicollinearity, and goodness-of-fit afterward.\nNaive Bayes (N-B)\nN-B is a probabilistic machine learning algorithm based on Bayes‚Äô Theorem. It‚Äôs particularly used for classification tasks, and it‚Äôs called ‚Äúnaive‚Äù because it assumes independence between features ‚Äî which is rarely true in practice, but surprisingly effective in many cases.\nFor example, in spam detection, N-B would assume that the presence of the word ‚Äúfree‚Äù is independent of ‚Äúmoney‚Äù ‚Äî given the email is spam. Not usually true, but this simplifies the math a lot.\nSure! Here‚Äôs a simple example of implementing Naive Bayes in R using the built-in iris dataset and the e1071 package.\n\n\nüì¶ Step 1: Install and load the required package\ninstall.packages(\"e1071\")  # Only run this once\nlibrary(e1071)\n\n\n\nüå∏ Step 2: Prepare the data\nWe‚Äôll use the famous iris dataset, which classifies flowers into three species.\ndata(iris)\n\n# Let's split the data into training and test sets\nset.seed(123)  # For reproducibility\nindexes &lt;- sample(1:nrow(iris), 0.7 * nrow(iris))\ntrain_data &lt;- iris[indexes, ]\ntest_data &lt;- iris[-indexes, ]\n\n\n\nüß† Step 3: Train the Naive Bayes model\nmodel &lt;- naiveBayes(Species ~ ., data = train_data)\nprint(model)\n\n\n\nüß™ Step 4: Make predictions\npredictions &lt;- predict(model, test_data)\n\n\n\nüìä Step 5: Evaluate the model\n{r}\n#| message: false\n#| warning: false\n#| output: true\n\nconfusion_matrix &lt;- table(Predicted = predictions, Actual = test_data$Species)\nprint(confusion_matrix)\n\naccuracy &lt;- sum(diag(confusion_matrix)) / sum(confusion_matrix)\ncat(\"Accuracy:\", round(accuracy * 100, 2), \"%\\n\")\n\nA confusion matrix is a table that visualizes the performance of a classification model by comparing its predicted labels to the true labels. It‚Äôs a valuable tool for understanding where a model might be getting confused and identifying areas for improvement.\nYou can explore Naive Bayes further with different data sets, feature selection, and using the caret or mlr3 packages in R for more structured workflows.\nSupport Vector Machines\nSVM is a supervised machine learning algorithm primarily used for classification, but it can also handle regression tasks. This means that\nAt its core, an SVM tries to find the best boundary (hyperplane) that separates data points of different classes in the feature space. SVM tries to maximize the margin between data points of different classes. The margin is the distance between the hyperplane and the nearest points from each class, which are called support vectors.\nA SVM for R might look like this:\n\n\n\nüì¶ Step 1: Install and load the required package\n\n# Install if not already installed\ninstall.packages(\"e1071\") # Only run this once\n\n# Downloading packages -------------------------------------------------------\n- Downloading e1071 from CRAN ...               OK [571.2 Kb in 0.49s]\n- Downloading proxy from CRAN ...               OK [169.3 Kb in 0.45s]\nSuccessfully downloaded 2 packages in 1.8 seconds.\n\nThe following package(s) will be installed:\n- e1071 [1.7-16]\n- proxy [0.4-27]\nThese packages will be installed into \"~/work/Machine-Learning-Clearly-Explained/Machine-Learning-Clearly-Explained/renv/library/R-4.2/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing proxy ...                          OK [installed binary and cached in 0.2s]\n- Installing e1071 ...                          OK [installed binary and cached in 0.22s]\nSuccessfully installed 2 packages in 0.48 seconds.\n\n# Load the package\nlibrary(e1071)\n\nYou might notice that the e1071 package is handy. You can use it for both N-B and SVM, and thus easily build and compare the two types of models.\nRandom Forests\nThese are essentially a collection of decision trees‚Äîhence the name ‚Äúforest.‚Äù Each tree is trained on a slightly different subset of the data, and their outputs are aggregated to make the final prediction.\nFor classification, the forest outputs the class that has the majority vote among the trees.\nFor regression, it averages the predictions of all the trees.\nBut what about it is ‚Äúrandom?‚Äù\n\nBootstrap sampling (Bagging): Each tree is trained on a random sample (with replacement) of the training data.\nRandom feature selection: When splitting nodes, each tree considers only a random subset of features instead of all features. This adds diversity and reduces correlation among trees.\n\nHere‚Äôs an example of using Random Forest in R using the built-in iris dataset. We‚Äôll use the randomForest package, a popular and simple way to build Random Forest models.\n\n\n‚úÖ Step-by-step Random Forest Example in R\n\n1. Install and load the package\ninstall.packages(\"randomForest\")  # Only run once library(randomForest) \n\n\n2. Load the data\ndata(iris) head(iris) \n\n\n3. Split the data into training and test sets\nset.seed(123)  # For reproducibility sample_index &lt;- sample(1:nrow(iris), 0.7 * nrow(iris))  # 70% train train_data &lt;- iris[sample_index, ] test_data &lt;- iris[-sample_index, ] \n\n\n4. Train the Random Forest model\nmodel &lt;- randomForest(Species ~ ., data = train_data, ntree = 100, mtry = 2, importance = TRUE) print(model) \n\n\n5. Predict on test data\npredictions &lt;- predict(model, newdata = test_data) \n\n\n6. Evaluate model accuracy\nconfusion_matrix &lt;- table(predicted = predictions, actual = test_data$Species) print(confusion_matrix) accuracy &lt;- sum(diag(confusion_matrix)) / sum(confusion_matrix) cat(\"Accuracy:\", accuracy) \n\n\n7. Plot feature importance\nimportance(model) varImpPlot(model) \nNeural Networks\nA Neural Networks (NN) machine learning model inspired by how the human brain works. It is composed of layers of interconnected nodes (called neurons) that can learn to recognize patterns in data.\nIn essence, a neural network is a function approximator that maps input data to an output (like predicting if a person has diabetes or classifying an image).\nHere‚Äôs a brief example of a neural network built in R using the neuralnet package to predict the logical AND function.\n\n\n\nüîß Prerequisites\nFirst, install and load the neuralnet package:\ninstall.packages(\"neuralnet\") library(neuralnet) \n\n\nüìä Prepare the Data\nCreate a simple dataset for the AND function:\n# Create data frame training_data &lt;- data.frame(   input1 = c(0, 0, 1, 1),   input2 = c(0, 1, 0, 1),   output = c(0, 0, 0, 1) ) \n\n\nüß† Train the Neural Network\n# Train neural network model &lt;- neuralnet(output ~ input1 + input2,                     data = training_data,                     hidden = 2,      # One hidden layer with 2 neurons                    linear.output = FALSE)  # Plot the network plot(model) \n\n\nüîç Make Predictions\n# Test the network predictions &lt;- compute(model, training_data[, c(\"input1\", \"input2\")]) predictions$net.result  # Output probabilities \nThis is a basic example. For more complex datasets, consider using keras in R for deep learning. Deep learning is a subset of machine learning that uses neural networks with many layers (hence ‚Äúdeep‚Äù) to model complex patterns in data. It excels at learning from large volumes of unstructured data such as images, audio, text, and video.\nHere‚Äôs a simple example of a deep learning model in R using the keras package. This example uses the classic MNIST handwritten digit classification problem.\n\n\nStep-by-step Example in R:\n\n1. Install and load the necessary packages\ninstall.packages(\"keras\") library(keras) \n\n\n2. Load the MNIST dataset\nmnist &lt;- dataset_mnist() x_train &lt;- mnist$train$x y_train &lt;- mnist$train$y x_test &lt;- mnist$test$x y_test &lt;- mnist$test$y \n\n\n3. Preprocess the data\n# Reshape and normalize input data x_train &lt;- array_reshape(x_train, c(nrow(x_train), 784)) / 255 x_test &lt;- array_reshape(x_test, c(nrow(x_test), 784)) / 255  # One-hot encode the labels y_train &lt;- to_categorical(y_train, 10) y_test &lt;- to_categorical(y_test, 10) \n\n\n4. Define the deep learning model\nmodel &lt;- keras_model_sequential() %&gt;%   layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %&gt;%   layer_dropout(rate = 0.4) %&gt;%   layer_dense(units = 128, activation = 'relu') %&gt;%   layer_dropout(rate = 0.3) %&gt;%   layer_dense(units = 10, activation = 'softmax') \n\n\n5. Compile the model\nmodel %&gt;% compile(   loss = 'categorical_crossentropy',   optimizer = optimizer_adam(),   metrics = c('accuracy') ) \n\n\n6. Train the model\nhistory &lt;- model %&gt;% fit(   x_train, y_train,   epochs = 30,   batch_size = 128,   validation_split = 0.2 ) \n\n\n7. Evaluate the model\nscore &lt;- model %&gt;% evaluate(x_test, y_test) cat('Test loss:', score$loss, '\\n') cat('Test accuracy:', score$accuracy, '\\n') \nThis will train a neural network with two hidden layers on the MNIST dataset."
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Linear and Logistic Regression",
    "section": "",
    "text": "Regression is a way to find a relationship between variables and use that relationship to make predictions.\nFor example:\n\nIf you know the size of a house, regression can help predict its price.\nIf you know a student‚Äôs study hours, regression can estimate their exam score.\n\nIt‚Äôs like drawing a trend line through data points to see how one thing changes with another. üìàüòä\nLinear regression is one of the most useful and popular forecasting tools available. Quite simply, it assumes that there is a linear relationship between one variable and a second variable. If you plot data for these two variables on a graph, you can draw a line through it and make a decent guess about what the second variable will be depending on the value of the first variable.\nTo learn more technical aspects of linear regression, visit the Wikipedia entry.\nAn initial model for ‚Äúsimple linear regression‚Äù is easy to build in R. This may not seem that impressive at first, but we‚Äôll examine its technical output shortly.\nLoad the data set and build a linear model:\n\n# Load dataset\ndata(iris)\n\n# Iris is a common data solution for testing models because of its relative simplicity\n# Fit linear regression model\nmodel &lt;- lm(Sepal.Length ~ Petal.Length, data = iris)\n\nLoad the necessary library for the chart:\n\n# ggplot2 makes great charts!\ninstall.packages('ggplot2')\nlibrary(ggplot2)\n\nNow build the chart that displays the relationship between variables:\n\n# Create scatter plot with regression line\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) +\n  geom_point(color = \"blue\", alpha = 0.6) +  # Scatter points\n  geom_smooth(method = \"lm\", color = \"red\", fill = \"pink\", se = TRUE) +  # Regression line\n  labs(title = \"Relationship Between Sepal Length and Petal Length\",\n       subtitle = \"A simple linear regression model\",\n       x = \"Petal Length\",\n       y = \"Sepal Length\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe variable Petal Length is on the x-axis (horizontal) and is called the ‚Äúindependent‚Äù variable. The variable Sepal Length is on the y-axis (vertical) and is called the ‚Äúdependent‚Äù variable. As you might guess, the objective of this ‚Äúmodel‚Äù (a simulation of reality) is to see how much influence Petal Length may have on Sepal Length. We don‚Äôt know if they are related at all at first, but by examining the data we can tease out that perhaps we can draw a straight line through the data that explains a possible relationship. In this case we see that, generally speaking, the longer the petal length is on an iris, the longer sepal length will be. Sometimes these relationships are intuitive, such as a taller person being heavier than a short person, but not always. Generally, we use data science to look for non-obvious relationships and performing a linear regression is just one tool.\nNow that we have an intuitive grasp on the subject from the visualization, let‚Äôs look at the statistical facts about this model.\n\n# Display model summary\nsummary(model)\n\nLogistic regression is also quite useful. It is a statistical method used for binary classification, where the outcome variable has two possible values (e.g., 0 or 1, yes or no, true or false). It models the probability of one class occurring using the logistic (sigmoid) function, which maps any real-valued input to a value between 0 and 1.\nLogistic regression is commonly used in fields like medicine, finance, and social sciences for tasks such as disease prediction, credit scoring, and customer retention analysis.\nLet‚Äôs predict whether a car has automatic (am = 0) or manual (am = 1) transmission based on its miles per gallon (mpg):\n\n# Load dataset\ndata(mtcars)\n\n# Convert 'am' to a factor (0 = automatic, 1 = manual)\nmtcars$am &lt;- as.factor(mtcars$am)\n\n# Fit a logistic regression model\nmodel &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nglm(formula = am ~ mpg, family = binomial, data = mtcars)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5701  -0.7531  -0.4245   0.5866   2.0617  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -6.6035     2.3514  -2.808  0.00498 **\nmpg           0.3070     0.1148   2.673  0.00751 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 29.675  on 30  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe model predicts whether a car is manual (1) or automatic (0) based on mpg.\n\n\n\nThe coefficient of mpg tells us whether increasing mpg makes a car more likely to have a manual transmission.\n\n\n\nThe output is in log-odds, but we can convert it to probabilities.\n\nIn R 4.5.0, you can use the palmerpenguins dataset, which is a built-in dataset (after loading the appropriate package), to perform regression analysis. Here‚Äôs a simple example using linear regression to predict penguin body mass from flipper length.\nFirst, we‚Äôll make sure to have the palmerpenguins package installed and loaded, just in case someone has not updated to R4.5.0:\n\n# Install if you haven't already\ninstall.packages(\"palmerpenguins\")\n\n# Downloading packages -------------------------------------------------------\n- Downloading palmerpenguins from CRAN ...      OK [2.9 Mb in 0.63s]\nSuccessfully downloaded 1 package in 0.75 seconds.\n\nThe following package(s) will be installed:\n- palmerpenguins [0.1.1]\nThese packages will be installed into \"~/work/Machine-Learning-Clearly-Explained/Machine-Learning-Clearly-Explained/renv/library/R-4.2/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing palmerpenguins ...                 OK [installed binary and cached in 0.25s]\nSuccessfully installed 1 package in 0.29 seconds.\n\n# Load the package\nlibrary(palmerpenguins)\n\nThen we would build the model:\n\n# Load the data\ndata(\"penguins\")\n\n# Remove rows with missing values\npenguins_clean &lt;- na.omit(penguins)\n\n# Fit a linear regression model: body_mass_g ~ flipper_length_mm\nmodel &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins_clean)\n\n# Show summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm, data = penguins_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1057.33  -259.79   -12.24   242.97  1293.89 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -5872.09     310.29  -18.93   &lt;2e-16 ***\nflipper_length_mm    50.15       1.54   32.56   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 393.3 on 331 degrees of freedom\nMultiple R-squared:  0.7621,    Adjusted R-squared:  0.7614 \nF-statistic:  1060 on 1 and 331 DF,  p-value: &lt; 2.2e-16\n\n\n\nExplanation:\n\nlm() fits a linear regression model.\nbody_mass_g is the response variable.\nflipper_length_mm is the predictor.\n\nHere is how you could visualize the model:\n\n# Plot\nplot(penguins_clean$flipper_length_mm, penguins_clean$body_mass_g,\n     main = \"Body Mass vs Flipper Length\",\n     xlab = \"Flipper Length (mm)\",\n     ylab = \"Body Mass (g)\",\n     pch = 19, col = \"steelblue\")\n\n# Add regression line\nabline(model, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nWhat the model demonstrates is that for every 1 mm increase in flipper length, the predicted body mass increases by approximately 58.4 grams."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Bottom Line Up Front: As a strategic planner and software developer, my goal is data analysis that yields actionable insights.\n\nüëã Hi, I‚Äôm JT, aka @DSD-resilience on GitHub, because resilience is a one of the key values of my digital products firm, DSD.\nüëÄ I‚Äôm interested in Shiny apps, statistics, AI-ML, data storytelling, and cyber resiliency.\nüå± I‚Äôm always learning more R, Python and SQL.\nüíûÔ∏è I‚Äôm looking to collaborate on Shiny apps for enterprise, dashboards for strategic insights, statistical analysis and data cleaning.\nüì´ How to reach me: millerauthor@datascientistdude.com for business inquiries please.\nüòÑ Dad Joke of the Day: How do you throw a party in outer space? You plan-et!\n‚ö° Fun fact: I am a (former) Russian/Ukrainian linguist."
  }
]