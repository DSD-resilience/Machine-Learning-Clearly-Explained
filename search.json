[
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Linear and Logistic Regression",
    "section": "",
    "text": "Regression is a way to find a relationship between variables and use that relationship to make predictions.\nFor example:\n\nIf you know the size of a house, regression can help predict its price.\nIf you know a student‚Äôs study hours, regression can estimate their exam score.\n\nIt‚Äôs like drawing a trend line through data points to see how one thing changes with another. üìàüòä\nLinear regression is one of the most useful and popular forecasting tools available. Quite simply, it assumes that there is a linear relationship between one variable and a second variable. If you plot data for these two variables on a graph, you can draw a line through it and make a decent guess about what the second variable will be depending on the value of the first variable.\nTo learn more technical aspects of linear regression, visit the Wikipedia entry.\nAn initial model for ‚Äúsimple linear regression‚Äù is easy to build in R. This may not seem that impressive at first, but we‚Äôll examine its technical output shortly.\nLoad the data set and build a linear model:\n\n# Load dataset\ndata(iris)\n\n# Iris is a common data solution for testing models because of its relative simplicity\n# Fit linear regression model\nmodel &lt;- lm(Sepal.Length ~ Petal.Length, data = iris)\n\nLoad the necessary library for the chart:\n\n# ggplot2 makes great charts!\ninstall.packages('ggplot2')\nlibrary(ggplot2)\n\nNow build the chart that displays the relationship between variables:\n\n# Create scatter plot with regression line\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) +\n  geom_point(color = \"blue\", alpha = 0.6) +  # Scatter points\n  geom_smooth(method = \"lm\", color = \"red\", fill = \"pink\", se = TRUE) +  # Regression line\n  labs(title = \"Relationship Between Sepal Length and Petal Length\",\n       subtitle = \"A simple linear regression model\",\n       x = \"Petal Length\",\n       y = \"Sepal Length\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe variable Petal Length is on the x-axis (horizontal) and is called the ‚Äúindependent‚Äù variable. The variable Sepal Length is on the y-axis (vertical) and is called the ‚Äúdependent‚Äù variable. As you might guess, the objective of this ‚Äúmodel‚Äù (a simulation of reality) is to see how much influence Petal Length may have on Sepal Length. We don‚Äôt know if they are related at all at first, but by examining the data we can tease out that perhaps we can draw a straight line through the data that explains a possible relationship. In this case we see that, generally speaking, the longer the petal length is on an iris, the longer sepal length will be. Sometimes these relationships are intuitive, such as a taller person being heavier than a short person, but not always. Generally, we use data science to look for non-obvious relationships and performing a linear regression is just one tool.\nNow that we have an intuitive grasp on the subject from the visualization, let‚Äôs look at the statistical facts about this model.\n\n# Display model summary\nsummary(model)\n\nLogistic regression is also quite useful. It is a statistical method used for binary classification, where the outcome variable has two possible values (e.g., 0 or 1, yes or no, true or false). It models the probability of one class occurring using the logistic (sigmoid) function, which maps any real-valued input to a value between 0 and 1.\nLogistic regression is commonly used in fields like medicine, finance, and social sciences for tasks such as disease prediction, credit scoring, and customer retention analysis.\nLet‚Äôs predict whether a car has automatic (am = 0) or manual (am = 1) transmission based on its miles per gallon (mpg):\n\n# Load dataset\ndata(mtcars)\n\n# Convert 'am' to a factor (0 = automatic, 1 = manual)\nmtcars$am &lt;- as.factor(mtcars$am)\n\n# Fit a logistic regression model\nmodel &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nglm(formula = am ~ mpg, family = binomial, data = mtcars)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5701  -0.7531  -0.4245   0.5866   2.0617  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -6.6035     2.3514  -2.808  0.00498 **\nmpg           0.3070     0.1148   2.673  0.00751 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 29.675  on 30  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe model predicts whether a car is manual (1) or automatic (0) based on mpg.\n\n\n\nThe coefficient of mpg tells us whether increasing mpg makes a car more likely to have a manual transmission.\n\n\n\nThe output is in log-odds, but we can convert it to probabilities.\n\nHere is another example."
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is used when you want to classify your data but you are not sure how. Clustering is a form of unsupervised learning.\nOne of the most popular types of clustering models is K-means clustering.\nAssumes: Spherical clusters of equal variance. K-Means tends to group data into clusters where each cluster is roughly circular (in 2D) or spherical (in higher dimensions) around its centroid.\nThis is because K-Means uses Euclidean distance to assign points to the nearest cluster center ‚Äî and all points that are equidistant to a center form a hypersphere.\nüìå Key point- this makes clusters are shaped like blobs around the center ‚Äî not elongated or skewed.\nStrengths: Fast, simple, works well with large datasets.\nLimitations: Requires the number of clusters (k) to be specified; not great with non-spherical or varying-density clusters.\nR package: [stats::kmeans()], also cluster and factoextra for visualization.\nAnother popular type of clustering model is Hierarchical clustering.\nTypes: Agglomerative (bottom-up) and divisive (top-down).\nOutput: Dendrogram (tree of clusters).\nStrengths: Does not require predefined k; good for nested structures.\nLimitations: Computationally expensive for large datasets.\nR package: stats::hclust(), dendextend, cluster.\nStill another popular type of clustering mode is DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering.\n‚úÖ Why is it popular?\n\nThere is no need to specify the number of clusters in advance (unlike K-means).\nCan find arbitrarily shaped clusters, not just spherical ones.\nRobust to outliers ‚Äî it can identify noise points as a separate class.\n\nCore Concepts Œµ (epsilon): Radius around a point to look for neighboring points.\nMinPts: Minimum number of points within Œµ radius to form a dense region.\nCore Point: Has at least MinPts within Œµ.\nBorder Point: Within Œµ of a core point but has fewer than MinPts.\nNoise Point: Not a core or border point.\nLet‚Äôs do an example of DBSCAN. The iris data set is often used, even though it is already labeled.\n\n# Load necessary packages\ninstall.packages(\"dbscan\")\n\n# Downloading packages -------------------------------------------------------\n- Downloading dbscan from CRAN ...              OK [2.8 Mb in 0.5s]\n- Downloading generics from CRAN ...            OK [69.9 Kb in 0.27s]\n- Downloading Rcpp from CRAN ...                OK [2 Mb in 0.28s]\nSuccessfully downloaded 3 packages in 2.1 seconds.\n\nThe following package(s) will be installed:\n- dbscan   [1.2.2]\n- generics [0.1.3]\n- Rcpp     [1.0.14]\nThese packages will be installed into \"~/work/Machine-Learning-Clearly-Explained/Machine-Learning-Clearly-Explained/renv/library/R-4.2/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing generics ...                       OK [installed binary and cached in 0.22s]\n- Installing Rcpp ...                           OK [installed binary and cached in 0.45s]\n- Installing dbscan ...                         OK [installed binary and cached in 0.35s]\nSuccessfully installed 3 packages in 1.2 seconds.\n\ninstall.packages(\"ggplot2\")\n\n# Downloading packages -------------------------------------------------------\n- Downloading ggplot2 from CRAN ...             OK [4.7 Mb in 0.34s]\n- Downloading gtable from CRAN ...              OK [213 Kb in 0.26s]\n- Downloading isoband from CRAN ...             OK [1.6 Mb in 0.4s]\n- Downloading scales from CRAN ...              OK [682.7 Kb in 0.3s]\n- Downloading farver from CRAN ...              OK [1.4 Mb in 0.24s]\n- Downloading labeling from CRAN ...            OK [57.8 Kb in 0.21s]\n- Downloading munsell from CRAN ...             OK [235.8 Kb in 0.35s]\n- Downloading colorspace from CRAN ...          OK [2.5 Mb in 0.45s]\n- Downloading RColorBrewer from CRAN ...        OK [51.8 Kb in 0.42s]\n- Downloading viridisLite from CRAN ...         OK [1.2 Mb in 0.27s]\n- Downloading tibble from CRAN ...              OK [656 Kb in 0.25s]\n- Downloading fansi from CRAN ...               OK [299.4 Kb in 0.25s]\n- Downloading magrittr from CRAN ...            OK [215.3 Kb in 0.27s]\n- Downloading pillar from CRAN ...              OK [635.3 Kb in 0.26s]\n- Downloading utf8 from CRAN ...                OK [143.4 Kb in 0.27s]\n- Downloading vctrs from CRAN ...               OK [1.2 Mb in 0.25s]\n- Downloading pkgconfig from CRAN ...           OK [17.1 Kb in 0.29s]\n- Downloading withr from CRAN ...               OK [211.6 Kb in 0.28s]\nSuccessfully downloaded 18 packages in 7.7 seconds.\n\nThe following package(s) will be installed:\n- colorspace   [2.1-1]\n- fansi        [1.0.6]\n- farver       [2.1.2]\n- ggplot2      [3.5.2]\n- gtable       [0.3.6]\n- isoband      [0.2.7]\n- labeling     [0.4.3]\n- magrittr     [2.0.3]\n- munsell      [0.5.1]\n- pillar       [1.10.2]\n- pkgconfig    [2.0.3]\n- RColorBrewer [1.1-3]\n- scales       [1.3.0]\n- tibble       [3.2.1]\n- utf8         [1.2.4]\n- vctrs        [0.6.5]\n- viridisLite  [0.4.2]\n- withr        [3.0.2]\nThese packages will be installed into \"~/work/Machine-Learning-Clearly-Explained/Machine-Learning-Clearly-Explained/renv/library/R-4.2/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing gtable ...                         OK [installed binary and cached in 0.34s]\n- Installing isoband ...                        OK [installed binary and cached in 0.24s]\n- Installing farver ...                         OK [installed binary and cached in 0.23s]\n- Installing labeling ...                       OK [installed binary and cached in 0.2s]\n- Installing colorspace ...                     OK [installed binary and cached in 0.31s]\n- Installing munsell ...                        OK [installed binary and cached in 0.25s]\n- Installing RColorBrewer ...                   OK [installed binary and cached in 0.2s]\n- Installing viridisLite ...                    OK [installed binary and cached in 0.21s]\n- Installing scales ...                         OK [installed binary and cached in 0.38s]\n- Installing fansi ...                          OK [installed binary and cached in 0.21s]\n- Installing magrittr ...                       OK [installed binary and cached in 0.2s]\n- Installing utf8 ...                           OK [installed binary and cached in 0.2s]\n- Installing vctrs ...                          OK [installed binary and cached in 0.34s]\n- Installing pillar ...                         OK [installed binary and cached in 0.42s]\n- Installing pkgconfig ...                      OK [installed binary and cached in 0.2s]\n- Installing tibble ...                         OK [installed binary and cached in 0.46s]\n- Installing withr ...                          OK [installed binary and cached in 0.2s]\n- Installing ggplot2 ...                        OK [installed binary and cached in 0.65s]\nSuccessfully installed 18 packages in 5.7 seconds.\n\nlibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nlibrary(ggplot2)\n\n# Load the iris dataset and check out the first six observations\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# Use only the numeric features (remove the species label)\niris_data &lt;- iris[, 1:4]\n\n# Scale the data (DBSCAN is sensitive to scale)\niris_scaled &lt;- scale(iris_data)\n\n# Plot kNN distances to estimate a good value for eps\nkNNdistplot(iris_scaled, k = 4)\nabline(h = 0.6, lty = 2)  # use your visual judgment here for eps\n\n\n\n\n\n\n\n\nThis plot helps you choose a good eps value. You‚Äôre looking for the ‚Äúelbow‚Äù of the curve.\n\n# Run DBSCAN with eps and minPts\ndb &lt;- dbscan(iris_scaled, eps = 0.6, minPts = 4)\n\n# Check clustering results\ndb$cluster  # Cluster labels (0 means noise)\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 0 2 2 0 2 0 2 2 2 2 2 0 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 0 0 2 0 0 2\n[112] 2 2 2 2 2 2 0 0 0 2 2 0 2 2 0 2 2 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\n# Add cluster labels to the original data\niris$Cluster &lt;- as.factor(db$cluster)\n\n# Visualize the clusters (using first two PCA components for clarity)\npca &lt;- prcomp(iris_scaled)\niris_pca &lt;- data.frame(pca$x[,1:2], Cluster = iris$Cluster)\n\nggplot(iris_pca, aes(PC1, PC2, color = Cluster)) +\n  geom_point(size = 2) +\n  labs(title = \"DBSCAN Clustering on Iris Dataset (PCA Projection)\")\n\n\n\n\n\n\n\n\n\nüìù Notes\neps (epsilon): max distance between two samples to be considered neighbors.\nminPts: minimum number of points required to form a dense region.\nThe elbow/k-distance plot helps you pick a good eps."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Machine learning is one of the most important developments in computer science. It has driven modern progress in many ways, through improving medical research, construction methods, urban planning, and most any field that you can name that generates and consumes data.\nFundamentally, it is a way for computers to learn from data and improve at tasks without being explicitly told what to do step by step.\nInstead of writing rules by hand, we give the computer a lot of examples, and it figures out patterns or rules on its own.\nFor example:\nIf you want a computer to recognize pictures of cats, instead of coding all the features of a cat (like fur, ears, eyes, tail‚Ä¶), you show it many labeled pictures of cats and non-cats.\nOver time, it ‚Äúlearns‚Äù what features make a cat picture likely, based on those examples.\nBut that is not all. It depends on what models you utilize, but a computer doesn‚Äôt even need labels to recognize patterns in your data.\nIf you want a computer to tell you what marketing might work best, then you can give it the data from several customers. This is very common in today‚Äôs society and used in almost every industry.\nOver time, the computer ‚Äúlearns‚Äù what features are distinct from customer to customer and labels the customer by what marketing could work best and result in higher sales.\nBoth of these instances are kind of like teaching by example rather than instruction.\nMachine learning works because data contains patterns, and if you can model those patterns mathematically, you can generalize to new, unseen data.\nIt works well when:\nIn math terms, it‚Äôs about finding a function f(x) approximates y, where:\nTo apply machine learning, you usually need three things:"
  },
  {
    "objectID": "index.html#where-is-ml-useful",
    "href": "index.html#where-is-ml-useful",
    "title": "Introduction",
    "section": "üí° Where Is ML Useful?",
    "text": "üí° Where Is ML Useful?\nMachine learning is used almost everywhere you can collect data and automatize processes. Even something as human as farming is going to rely on better weather forecasting and production reports. Here are some broad categories:\n\nRegression ‚Äì predicting prices, forecasting demand\nClassification ‚Äì spam detection, disease diagnosis, fraud detection\nClustering ‚Äì segmenting customers, grouping similar documents\nRecommendation ‚Äì Netflix, YouTube, Amazon suggestions\nComputer Vision ‚Äì self-driving cars, facial recognition\nNatural Language Processing ‚Äì chatbots, translation, sentiment analysis"
  },
  {
    "objectID": "index.html#key-takeaway",
    "href": "index.html#key-takeaway",
    "title": "Introduction",
    "section": "üéØ Key Takeaway",
    "text": "üéØ Key Takeaway\n\nMachine learning is about using data to make smarter decisions!\nWith the right tools and data, we can teach computers to help us in ways that were once impossible ‚Äî from personalized healthcare to AI-powered assistants."
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification models are used when you know the categories, or labels, that you wish to use on your data. Both classification and regression are forms of supervised learning.\nSupervised learning is a type of machine learning where the model learns from a labeled data set‚Äîthat is, data that has both input features (X) and known target outputs (Y).\nThe goal is to learn a function that maps inputs to outputs so it can make predictions on new, unseen data.\nA practical example of supervised learning and classification is spam email detection. We want to automatically classify incoming emails as ‚Äúspam‚Äù or ‚Äúnot spam.‚Äù The inputs (features) could include:\n\nPresence of certain keywords (like ‚Äúfree‚Äù, ‚Äúwinner‚Äù, etc.)\nSender‚Äôs email address\nNumber of links or attachments\nEmail length\nFrequency of punctuation like ‚Äú!!!‚Äù or ‚Äú$$$‚Äù\n\nThese act like clues to let the computer know that a specific email could be spam. You can understand how this is modeled on human thinking. We also look for patterns that will lead us to a conclusion.\nThe outputs (labels) in this case would be 1 for spam, and 0 for not spam (aka ham). If you have email a classification model is likely at work on your account, separating the ‚Äúspam‚Äù from the ‚Äúham.‚Äù\nCommon supervised learning models used for classifying data are:\nLogistic Regression\nThis is a statistical method used to model the relationship between a binary dependent variable and one or more independent variables. An answer is either ‚Äúyes‚Äù or ‚Äúno.‚Äù It estimates the probability that the dependent variable equals a certain value (usually 1), using a logistic function (a sigmoid, or s-shaped, curve).\nTo run a logistic regression in R, use the glm() function with an argument of family = binomial. This fits a generalized linear model where the dependent variable is binary (0/1). You should check assumptions, multicollinearity, and goodness-of-fit afterward. Below is the basic structure in an example:\nNaive Bayes (N-B)\nN-B is a probabilistic machine learning algorithm based on Bayes‚Äô Theorem. It‚Äôs particularly used for classification tasks, and it‚Äôs called ‚Äúnaive‚Äù because it assumes independence between features ‚Äî which is rarely true in practice, but surprisingly effective in many cases.\nFor example, in spam detection, N-B would assume that the presence of the word ‚Äúfree‚Äù is independent of ‚Äúmoney‚Äù ‚Äî given the email is spam. Not usually true, but this simplifies the math a lot.\nSupport Vector Machines\nSVM is a supervised machine learning algorithm primarily used for classification, but it can also handle regression tasks. This means that\nAt its core, an SVM tries to find the best boundary (hyperplane) that separates data points of different classes in the feature space. SVM tries to maximize the margin between data points of different classes. The margin is the distance between the hyperplane and the nearest points from each class, which are called support vectors.\nRandom Forests\nThese are essentially a collection of decision trees‚Äîhence the name ‚Äúforest.‚Äù Each tree is trained on a slightly different subset of the data, and their outputs are aggregated to make the final prediction.\nFor classification, the forest outputs the class that has the majority vote among the trees.\nFor regression, it averages the predictions of all the trees.\nBut what about it is ‚Äúrandom?‚Äù\n\nBootstrap sampling (Bagging): Each tree is trained on a random sample (with replacement) of the training data.\nRandom feature selection: When splitting nodes, each tree considers only a random subset of features instead of all features. This adds diversity and reduces correlation among trees.\n\nNeural Networks"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Bottom Line Up Front: As a strategic planner and software developer, my goal is data analysis that yields actionable insights.\n\nüëã Hi, I‚Äôm JT, aka @DSD-resilience on GitHub, because resilience is a one of the key values of my digital products firm, DSD.\nüëÄ I‚Äôm interested in Shiny apps, statistics, AI-ML, data storytelling, and cyber resiliency.\nüå± I‚Äôm always learning more R, Python and SQL.\nüíûÔ∏è I‚Äôm looking to collaborate on Shiny apps for enterprise, dashboards for strategic insights, statistical analysis and data cleaning.\nüì´ How to reach me: millerauthor@datascientistdude.com for business inquiries please.\nüòÑ Dad Joke of the Day: How do you throw a party in outer space? You plan-et!\n‚ö° Fun fact: I am a (former) Russian/Ukrainian linguist."
  }
]