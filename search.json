[
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Linear and Logistic Regression",
    "section": "",
    "text": "Regression is a way to find a relationship between variables and use that relationship to make predictions.\nFor example:\n\nIf you know the size of a house, regression can help predict its price.\nIf you know a student‚Äôs study hours, regression can estimate their exam score.\n\nIt‚Äôs like drawing a trend line through data points to see how one thing changes with another. üìàüòä\nLinear regression is one of the most useful and popular forecasting tools available. Quite simply, it assumes that there is a linear relationship between one variable and a second variable. If you plot data for these two variables on a graph, you can draw a line through it and make a decent guess about what the second variable will be depending on the value of the first variable.\nTo learn more technical aspects of linear regression, visit the Wikipedia entry.\nAn initial model for ‚Äúsimple linear regression‚Äù is easy to build in R. This may not seem that impressive at first, but we‚Äôll examine its technical output shortly.\nLoad the data set and build a linear model:\n\n# Load dataset\ndata(iris)\n\n# Fit linear regression model\nmodel &lt;- lm(Sepal.Length ~ Petal.Length, data = iris)\n\nLoad the necessary library for the chart:\n\n# ggplot2 makes great charts!\ninstall.packages('ggplot2')\nlibrary(ggplot2)\n\nNow build the chart that displays the relationship between variables:\n\n# Create scatter plot with regression line\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) +\n  geom_point(color = \"blue\", alpha = 0.6) +  # Scatter points\n  geom_smooth(method = \"lm\", color = \"red\", fill = \"pink\", se = TRUE) +  # Regression line\n  labs(title = \"Relationship Between Sepal Length and Petal Length\",\n       subtitle = \"A simple linear regression model\",\n       x = \"Petal Length\",\n       y = \"Sepal Length\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe variable Petal Length is on the x-axis (horizontal) and is called the ‚Äúindependent‚Äù variable. The variable Sepal Length is on the y-axis (vertical) and is called the ‚Äúdependent‚Äù variable. As you might guess, the objective of this ‚Äúmodel‚Äù (a simulation of reality) is to see how much influence Petal Length may have on Sepal Length. We don‚Äôt know if they are related at all at first, but by examining the data we can tease out that perhaps we can draw a straight line through the data that explains a possible relationship. In this case we see that, generally speaking, the longer the petal length is on an iris, the longer sepal length will be. Sometimes these relationships are intuitive, such as a taller person being heavier than a short person, but not always. Generally, we use data science to look for non-obvious relationships and performing a linear regression is just one tool.\nNow that we have an intuitive grasp on the subject from the visualization, let‚Äôs look at the statistical facts about this model.\n\n# Display model summary\nsummary(model)\n\nLogistic regression is also quite useful. It is a statistical method used for binary classification, where the outcome variable has two possible values (e.g., 0 or 1, yes or no, true or false). It models the probability of one class occurring using the logistic (sigmoid) function, which maps any real-valued input to a value between 0 and 1.\nLogistic regression is commonly used in fields like medicine, finance, and social sciences for tasks such as disease prediction, credit scoring, and customer retention analysis.\nLet‚Äôs predict whether a car has automatic (am = 0) or manual (am = 1) transmission based on its miles per gallon (mpg):\n\n# Load dataset\ndata(mtcars)\n\n# Convert 'am' to a factor (0 = automatic, 1 = manual)\nmtcars$am &lt;- as.factor(mtcars$am)\n\n# Fit a logistic regression model\nmodel &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nglm(formula = am ~ mpg, family = binomial, data = mtcars)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5701  -0.7531  -0.4245   0.5866   2.0617  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -6.6035     2.3514  -2.808  0.00498 **\nmpg           0.3070     0.1148   2.673  0.00751 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 29.675  on 30  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe model predicts whether a car is manual (1) or automatic (0) based on mpg.\n\n\n\nThe coefficient of mpg tells us whether increasing mpg makes a car more likely to have a manual transmission.\n\n\n\nThe output is in log-odds, but we can convert it to probabilities."
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is used when you want to classify your data but you are not sure how. Clustering is a form of unsupervised learning.\nOne type of popular clustering model is K-means clustering.\nAssumes: Spherical clusters of equal variance.\nStrengths: Fast, simple, works well with large datasets.\nLimitations: Requires the number of clusters (k) to be specified; not great with non-spherical or varying-density clusters.\nR package: [stats::kmeans()], also cluster and factoextra for visualization.\nAnother popular type of clustering model is Hierarchical clustering.\nTypes: Agglomerative (bottom-up) and divisive (top-down).\nOutput: Dendrogram (tree of clusters).\nStrengths: Does not require predefined k; good for nested structures.\nLimitations: Computationally expensive for large datasets.\nR package: stats::hclust(), dendextend, cluster."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Machine learning is one of the most important developments in computer science. It has driven modern progress in many ways, spurring progress in medical research, construction, urban planning, and most any field that you can name.\nFundamentally, it is a way for computers to learn from data and improve at tasks without being explicitly told what to do step by step.\nInstead of writing rules by hand, we give the computer a lot of examples, and it figures out patterns or rules on its own.\nFor example:\nIf you want a computer to recognize pictures of cats, instead of coding all the features of a cat (like fur, ears, tail‚Ä¶), you show it many labeled pictures of cats and non-cats.\nOver time, it ‚Äúlearns‚Äù what features make a cat picture likely, based on those examples.\nBut that is not all. It depends on what models you utilize, but a computer doesn‚Äôt even need labels to recognize patterns in your data.\nIf you want a computer to tell you what marketing might work best, then you can give it the data from several customers. This is very common in today‚Äôs society and used in almost every industry.\nOver time, the computer ‚Äúlearns‚Äù what features are distinct from customer to customer and labels the customer by what marketing could work best and result in higher sales.\nBoth of these instances are kind of like teaching by example rather than instruction."
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification models are used when you know the categories, or labels, that you wish to use on your data. Both classification and regression are forms of supervised learning."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Bottom Line Up Front: As a strategic planner and software developer, my goal is data analysis that yields actionable insights.\n\nüëã Hi, I‚Äôm JT, aka @DSD-resilience on GitHub, because resilience is a one of the key values of my digital products firm, DSD.\nüëÄ I‚Äôm interested in Shiny apps, statistics, AI-ML, data storytelling, and cyber resiliency.\nüå± I‚Äôm always learning more R, Python and SQL.\nüíûÔ∏è I‚Äôm looking to collaborate on Shiny apps for enterprise, dashboards for strategic insights, statistical analysis and data cleaning.\nüì´ How to reach me: millerauthor@datascientistdude.com for business inquiries please.\nüòÑ Dad Joke of the Day: How do you throw a party in outer space? You plan-et!\n‚ö° Fun fact: I am a (former) Russian/Ukrainian linguist."
  }
]